<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Speech Emotion Recognition - Kuan-Chen Chen</title>
    <link rel="stylesheet" href="project-style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
</head>
<body>
    <nav class="navbar">
        <div class="container">
            <a href="../index.html" class="back-link">
                <i class="fas fa-arrow-left"></i>
                <span data-en="Back to Portfolio" data-zh="ËøîÂõû‰ΩúÂìÅÈõÜ">Back to Portfolio</span>
            </a>
            <button class="lang-toggle" id="langToggle">
                <span class="lang-en">‰∏≠Êñá</span>
                <span class="lang-zh" style="display:none;">EN</span>
            </button>
        </div>
    </nav>

    <section class="project-header">
        <div class="container">
            <h1 class="project-title">Speech Emotion Recognition System</h1>
            <p class="project-subtitle" data-en="CNN-LSTM Hybrid Architecture for Emotion Classification" data-zh="CNN-LSTM Ê∑∑ÂêàÊû∂ÊßãÁöÑÊÉÖÁ∑íÂàÜÈ°û">
                CNN-LSTM Hybrid Architecture for Emotion Classification
            </p>
            <div class="project-meta">
                <div class="meta-item"><i class="fas fa-calendar"></i><span>Fall 2024</span></div>
                <div class="meta-item"><i class="fas fa-graduation-cap"></i><span>University of Florida</span></div>
                <div class="meta-item"><i class="fas fa-chart-line"></i><span data-en="87.41% Accuracy" data-zh="87.41% Ê∫ñÁ¢∫Áéá">87.41% Accuracy</span></div>
            </div>
            <div class="project-tags">
                <span class="tag">PyTorch</span>
                <span class="tag">CNN-LSTM</span>
                <span class="tag">Deep Learning</span>
                <span class="tag">Audio Processing</span>
                <span class="tag">Time Series</span>
            </div>
        </div>
    </section>

    <div class="project-content">
        <div class="container">
            <section class="content-section">
                <h2 class="section-title" data-en="Project Overview" data-zh="Â∞àÊ°àÊ¶ÇËø∞">Project Overview</h2>
                <p class="content-text" data-en="Developed a deep learning system that classifies emotional content from speech audio into 5 emotion categories using a hybrid CNN-LSTM architecture. The system processes 3-second audio clips at 48kHz, extracts mel-spectrogram features, and achieves 87.41% test accuracy through advanced data augmentation and regularization techniques. This project demonstrates proficiency in audio signal processing, deep learning architecture design, and PyTorch implementation." data-zh="ÈñãÁôºÊ∑±Â∫¶Â≠∏ÁøíÁ≥ªÁµ±Ôºå‰ΩøÁî® CNN-LSTM Ê∑∑ÂêàÊû∂ÊßãÂ∞áË™ûÈü≥Èü≥Ë®äÊÉÖÁ∑íÂÖßÂÆπÂàÜÈ°ûÁÇ∫ 5 Á®ÆÊÉÖÁ∑íÈ°ûÂà•„ÄÇÁ≥ªÁµ±ËôïÁêÜ 48kHz ÁöÑ 3 ÁßíÈü≥Ë®äÁâáÊÆµÔºåÊèêÂèñ mel-spectrogram ÁâπÂæµÔºå‰∏¶ÈÄèÈÅéÈÄ≤ÈöéÊï∏ÊìöÂ¢ûÂº∑ÂíåÊ≠£ÂâáÂåñÊäÄË°ìÈÅîÂà∞ 87.41% Ê∏¨Ë©¶Ê∫ñÁ¢∫Áéá„ÄÇÊ≠§Â∞àÊ°àÂ±ïÁèæÈü≥Ë®ä‰ø°ËôüËôïÁêÜ„ÄÅÊ∑±Â∫¶Â≠∏ÁøíÊû∂ÊßãË®≠Ë®àÂíå PyTorch ÂØ¶‰ΩúÁöÑÂ∞àÊ•≠ËÉΩÂäõ„ÄÇ">
                    Developed a deep learning system that classifies emotional content from speech audio into 5 emotion categories using a hybrid CNN-LSTM architecture. The system processes 3-second audio clips at 48kHz, extracts mel-spectrogram features, and achieves 87.41% test accuracy through advanced data augmentation and regularization techniques. This project demonstrates proficiency in audio signal processing, deep learning architecture design, and PyTorch implementation.
                </p>
            </section>

            <section class="content-section">
                <h2 class="section-title" data-en="Key Achievements" data-zh="‰∏ªË¶ÅÊàêÂ∞±">Key Achievements</h2>
                <ul class="highlights-list">
                    <li data-en="Achieved 87.41% test accuracy on 5-class emotion classification, outperforming Random Forest baseline by 9.6%" data-zh="Âú® 5 È°ûÊÉÖÁ∑íÂàÜÈ°û‰∏≠ÈÅîÂà∞ 87.41% Ê∏¨Ë©¶Ê∫ñÁ¢∫ÁéáÔºåË∂ÖË∂äÈö®Ê©üÊ£ÆÊûóÂü∫Ê∫ñ 9.6%">Achieved 87.41% test accuracy on 5-class emotion classification, outperforming Random Forest baseline by 9.6%</li>
                    <li data-en="Designed and implemented hybrid CNN-LSTM architecture with 2.75M parameters, combining spatial and temporal feature extraction" data-zh="Ë®≠Ë®à‰∏¶ÂØ¶‰ΩúÂÖ∑Êúâ 275 Ëê¨ÂèÉÊï∏ÁöÑ CNN-LSTM Ê∑∑ÂêàÊû∂ÊßãÔºåÁµêÂêàÁ©∫ÈñìÂíåÊôÇÂ∫èÁâπÂæµÊèêÂèñ">Designed and implemented hybrid CNN-LSTM architecture with 2.75M parameters, combining spatial and temporal feature extraction</li>
                    <li data-en="Implemented 5x data augmentation strategy (Gaussian noise, time shifting, volume scaling) preventing overfitting despite model complexity" data-zh="ÂØ¶‰Ωú 5 ÂÄçÊï∏ÊìöÂ¢ûÂº∑Á≠ñÁï•ÔºàÈ´òÊñØÈõúË®ä„ÄÅÊôÇÈñì‰ΩçÁßª„ÄÅÈü≥ÈáèÁ∏ÆÊîæÔºâÔºåÂú®Ê®°ÂûãË§áÈõúÂ∫¶‰∏ãÈò≤Ê≠¢ÈÅéÊì¨Âêà">Implemented 5x data augmentation strategy (Gaussian noise, time shifting, volume scaling) preventing overfitting despite model complexity</li>
                    <li data-en="Processed mel-spectrogram features (128 mel bands √ó 282 time frames) using librosa for robust audio representation" data-zh="‰ΩøÁî® librosa ËôïÁêÜ mel-spectrogram ÁâπÂæµÔºà128 ÂÄã mel Â∏∂ √ó 282 ÂÄãÊôÇÈñìÂπÄÔºâÔºåÂØ¶ÁèæÂº∑ÂÅ•ÁöÑÈü≥Ë®äË°®Á§∫">Processed mel-spectrogram features (128 mel bands √ó 282 time frames) using librosa for robust audio representation</li>
                </ul>
            </section>

            <section class="content-section">
                <h2 class="section-title" data-en="CNN-LSTM Hybrid Architecture" data-zh="CNN-LSTM Ê∑∑ÂêàÊû∂Êßã">CNN-LSTM Hybrid Architecture</h2>
                <p class="content-text" data-en="The model combines Convolutional Neural Networks for spatial feature extraction from mel-spectrograms with Bidirectional LSTM layers for temporal dependency modeling:" data-zh="Ê®°ÂûãÁµêÂêàÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑ØÂæû mel-spectrograms ÊèêÂèñÁ©∫ÈñìÁâπÂæµÔºå‰ª•ÂèäÈõôÂêë LSTM Â±§ÈÄ≤Ë°åÊôÇÂ∫è‰æùË≥¥Âª∫Ê®°Ôºö">
                    The model combines Convolutional Neural Networks for spatial feature extraction from mel-spectrograms with Bidirectional LSTM layers for temporal dependency modeling:
                </p>
                <div class="features-grid">
                    <div class="feature-card">
                        <h4 data-en="CNN Feature Extraction (3 Layers)" data-zh="CNN ÁâπÂæµÊèêÂèñÔºà3 Â±§Ôºâ">üî∑ CNN Feature Extraction</h4>
                        <p data-en="Three convolutional layers with batch normalization and max pooling progressively extract hierarchical spatial features from mel-spectrograms. Architecture: Conv2d(1‚Üí32)‚ÜíConv2d(32‚Üí64)‚ÜíConv2d(64‚Üí128) with 3√ó3 kernels, reducing dimensions from (1,128,282) to (128,16,35)." data-zh="‰∏âÂÄãÂç∑Á©çÂ±§ÈÖçÂêàÊâπÊ¨°Ê≠£Ë¶èÂåñÂíåÊúÄÂ§ßÊ±†ÂåñÔºåÈÄêÊ≠•Âæû mel-spectrograms ÊèêÂèñÂàÜÂ±§Á©∫ÈñìÁâπÂæµ„ÄÇÊû∂ÊßãÔºöConv2d(1‚Üí32)‚ÜíConv2d(32‚Üí64)‚ÜíConv2d(64‚Üí128)Ôºå‰ΩøÁî® 3√ó3 Ê†∏ÂøÉÔºåÂ∞áÁ∂≠Â∫¶Âæû (1,128,282) Á∏ÆÊ∏õËá≥ (128,16,35)„ÄÇ">
                            Three convolutional layers with batch normalization and max pooling progressively extract hierarchical spatial features from mel-spectrograms. Architecture: Conv2d(1‚Üí32)‚ÜíConv2d(32‚Üí64)‚ÜíConv2d(64‚Üí128) with 3√ó3 kernels, reducing dimensions from (1,128,282) to (128,16,35).
                        </p>
                    </div>
                    <div class="feature-card">
                        <h4 data-en="Bidirectional LSTM (2 Layers)" data-zh="ÈõôÂêë LSTMÔºà2 Â±§Ôºâ">üîÑ Bidirectional LSTM</h4>
                        <p data-en="Bidirectional LSTM with 128 hidden units per direction processes temporal sequences in both forward and backward directions, capturing long-range dependencies in emotional speech patterns. Outputs 256-dimensional feature vectors per time step." data-zh="ÈõôÂêë LSTM ÊØèÂÄãÊñπÂêëÂÖ∑Êúâ 128 ÂÄãÈö±ËóèÂñÆÂÖÉÔºåÂú®Ê≠£ÂêëÂíåÂèçÂêëËôïÁêÜÊôÇÂ∫èÂ∫èÂàóÔºåÊçïÊçâÊÉÖÁ∑íË™ûÈü≥Ê®°Âºè‰∏≠ÁöÑÈï∑Á®ã‰æùË≥¥„ÄÇÊØèÂÄãÊôÇÈñìÊ≠•Ëº∏Âá∫ 256 Á∂≠ÁâπÂæµÂêëÈáè„ÄÇ">
                            Bidirectional LSTM with 128 hidden units per direction processes temporal sequences in both forward and backward directions, capturing long-range dependencies in emotional speech patterns. Outputs 256-dimensional feature vectors per time step.
                        </p>
                    </div>
                    <div class="feature-card">
                        <h4 data-en="Regularization Strategy" data-zh="Ê≠£ÂâáÂåñÁ≠ñÁï•">üõ°Ô∏è Regularization Strategy</h4>
                        <p data-en="Multiple dropout layers (30% rate) after CNN, LSTM, and FC layers prevent overfitting. Combined with weight decay (1e-5) and learning rate scheduling (ReduceLROnPlateau), achieves stable training with ~99% train accuracy while maintaining 87.41% test accuracy." data-zh="Âú® CNN„ÄÅLSTM ÂíåÂÖ®ÈÄ£Êé•Â±§ÂæåË®≠ÁΩÆÂ§öÂÄã dropout Â±§Ôºà30% ÊØîÁéáÔºâÈò≤Ê≠¢ÈÅéÊì¨Âêà„ÄÇÁµêÂêàÊ¨äÈáçË°∞Ê∏õÔºà1e-5ÔºâÂíåÂ≠∏ÁøíÁéáË™øÂ∫¶ÔºàReduceLROnPlateauÔºâÔºåÂú®Ë®ìÁ∑¥Ê∫ñÁ¢∫ÁéáÁ¥Ñ 99% ÁöÑÂêåÊôÇÁ∂≠ÊåÅ 87.41% Ê∏¨Ë©¶Ê∫ñÁ¢∫Áéá„ÄÇ">
                            Multiple dropout layers (30% rate) after CNN, LSTM, and FC layers prevent overfitting. Combined with weight decay (1e-5) and learning rate scheduling (ReduceLROnPlateau), achieves stable training with ~99% train accuracy while maintaining 87.41% test accuracy.
                        </p>
                    </div>
                    <div class="feature-card">
                        <h4 data-en="Fully Connected Classifier" data-zh="ÂÖ®ÈÄ£Êé•ÂàÜÈ°ûÂô®">üìä Fully Connected Classifier</h4>
                        <p data-en="Two fully connected layers (256‚Üí128‚Üí5) with ReLU activation and dropout transform LSTM outputs into final emotion predictions. Cross-entropy loss function optimizes 5-class classification with balanced class support." data-zh="ÂÖ©ÂÄãÂÖ®ÈÄ£Êé•Â±§Ôºà256‚Üí128‚Üí5ÔºâÈÖçÂêà ReLU ÊøÄÊ¥ªÂíå dropoutÔºåÂ∞á LSTM Ëº∏Âá∫ËΩâÊèõÁÇ∫ÊúÄÁµÇÊÉÖÁ∑íÈ†êÊ∏¨„ÄÇ‰∫§ÂèâÁÜµÊêçÂ§±ÂáΩÊï∏ÂÑ™ÂåñÂÖ∑ÊúâÂπ≥Ë°°È°ûÂà•ÊîØÊåÅÁöÑ 5 È°ûÂàÜÈ°û„ÄÇ">
                            Two fully connected layers (256‚Üí128‚Üí5) with ReLU activation and dropout transform LSTM outputs into final emotion predictions. Cross-entropy loss function optimizes 5-class classification with balanced class support.
                        </p>
                    </div>
                </div>
            </section>

            <section class="content-section">
                <h2 class="section-title" data-en="Audio Signal Processing Pipeline" data-zh="Èü≥Ë®ä‰ø°ËôüËôïÁêÜÊµÅÁ®ã">Audio Signal Processing Pipeline</h2>
                <p class="content-text" data-en="Raw audio data undergoes sophisticated preprocessing to extract meaningful features for emotion recognition:" data-zh="ÂéüÂßãÈü≥Ë®äÊï∏ÊìöÁ∂ìÈÅéË§áÈõúÁöÑÈ†êËôïÁêÜ‰ª•ÊèêÂèñÊÉÖÁ∑íË≠òÂà•ÁöÑÊúâÊÑèÁæ©ÁâπÂæµÔºö">
                    Raw audio data undergoes sophisticated preprocessing to extract meaningful features for emotion recognition:
                </p>
                <div class="features-grid">
                    <div class="feature-card">
                        <h4 data-en="Mel-Spectrogram Extraction" data-zh="Mel-Spectrogram ÊèêÂèñ">üéµ Mel-Spectrogram Extraction</h4>
                        <p data-en="3-second audio clips (144,000 samples at 48kHz) converted to mel-spectrograms using librosa. Configuration: 1024 FFT window, 512 hop length, 128 mel bands, 8kHz max frequency. Results in (128√ó282) 2D representation capturing both frequency and temporal patterns." data-zh="3 ÁßíÈü≥Ë®äÁâáÊÆµÔºà48kHz ÁöÑ 144,000 ÂÄãÊ®£Êú¨Ôºâ‰ΩøÁî® librosa ËΩâÊèõÁÇ∫ mel-spectrograms„ÄÇÈÖçÁΩÆÔºö1024 FFT Á™óÂè£„ÄÅ512 Ë∑≥Ë∫çÈï∑Â∫¶„ÄÅ128 ÂÄã mel Â∏∂„ÄÅ8kHz ÊúÄÂ§ßÈ†ªÁéá„ÄÇÁî¢Áîü (128√ó282) ‰∫åÁ∂≠Ë°®Á§∫ÔºåÊçïÊçâÈ†ªÁéáÂíåÊôÇÂ∫èÊ®°Âºè„ÄÇ">
                            3-second audio clips (144,000 samples at 48kHz) converted to mel-spectrograms using librosa. Configuration: 1024 FFT window, 512 hop length, 128 mel bands, 8kHz max frequency. Results in (128√ó282) 2D representation capturing both frequency and temporal patterns.
                        </p>
                    </div>
                    <div class="feature-card">
                        <h4 data-en="Data Augmentation (5x)" data-zh="Êï∏ÊìöÂ¢ûÂº∑Ôºà5 ÂÄçÔºâ">üîÑ Data Augmentation (5x)</h4>
                        <p data-en="Three augmentation techniques applied with >30% probability: Gaussian noise (factor=0.008), time shifting (up to 25%), and volume scaling (0.7-1.3x). Multiplies effective training data from 630 to 3,150 samples per epoch, crucial for preventing overfitting with 2.75M parameters." data-zh="‰∏âÁ®ÆÂ¢ûÂº∑ÊäÄË°ì‰ª• >30% Ê©üÁéáÊáâÁî®ÔºöÈ´òÊñØÈõúË®äÔºàÂõ†Â≠ê=0.008Ôºâ„ÄÅÊôÇÈñì‰ΩçÁßªÔºàÊúÄÂ§ö 25%ÔºâÂíåÈü≥ÈáèÁ∏ÆÊîæÔºà0.7-1.3 ÂÄçÔºâ„ÄÇÂ∞áÊúâÊïàË®ìÁ∑¥Êï∏ÊìöÂæûÊØèÂÄã epoch ÁöÑ 630 ÂÄãÊ®£Êú¨Â¢ûÂä†Âà∞ 3,150 ÂÄãÔºåÂ∞çÊñºÈò≤Ê≠¢ 275 Ëê¨ÂèÉÊï∏ÁöÑÈÅéÊì¨ÂêàËá≥ÈóúÈáçË¶Å„ÄÇ">
                            Three augmentation techniques applied with >30% probability: Gaussian noise (factor=0.008), time shifting (up to 25%), and volume scaling (0.7-1.3x). Multiplies effective training data from 630 to 3,150 samples per epoch, crucial for preventing overfitting with 2.75M parameters.
                        </p>
                    </div>
                    <div class="feature-card">
                        <h4 data-en="Stratified Data Splitting" data-zh="ÂàÜÂ±§Êï∏ÊìöÂàÜÂâ≤">üìÇ Stratified Data Splitting</h4>
                        <p data-en="900 total samples split using stratified sampling: 70% training (630), 15% validation (135), 15% test (135). Ensures balanced class distribution across all splits, preventing class imbalance issues during training and evaluation." data-zh="900 ÂÄãÁ∏ΩÊ®£Êú¨‰ΩøÁî®ÂàÜÂ±§Êé°Ê®£ÂàÜÂâ≤Ôºö70% Ë®ìÁ∑¥Ôºà630Ôºâ„ÄÅ15% È©óË≠âÔºà135Ôºâ„ÄÅ15% Ê∏¨Ë©¶Ôºà135Ôºâ„ÄÇÁ¢∫‰øùÊâÄÊúâÂàÜÂâ≤‰∏≠ÁöÑÂπ≥Ë°°È°ûÂà•ÂàÜÂ∏ÉÔºåÈò≤Ê≠¢Ë®ìÁ∑¥ÂíåË©ï‰º∞ÊúüÈñìÁöÑÈ°ûÂà•‰∏çÂπ≥Ë°°ÂïèÈ°å„ÄÇ">
                            900 total samples split using stratified sampling: 70% training (630), 15% validation (135), 15% test (135). Ensures balanced class distribution across all splits, preventing class imbalance issues during training and evaluation.
                        </p>
                    </div>
                    <div class="feature-card">
                        <h4 data-en="Log-Scale Normalization" data-zh="Â∞çÊï∏Â∞∫Â∫¶Ê≠£Ë¶èÂåñ">üìè Log-Scale Normalization</h4>
                        <p data-en="Mel-spectrograms converted to decibel (dB) scale using log transformation, normalizing dynamic range and emphasizing perceptually relevant features. Standard practice in audio ML for improved model convergence and performance." data-zh="Mel-spectrograms ‰ΩøÁî®Â∞çÊï∏ËΩâÊèõËΩâÊèõÁÇ∫ÂàÜË≤ùÔºàdBÔºâÂ∞∫Â∫¶ÔºåÊ≠£Ë¶èÂåñÂãïÊÖãÁØÑÂúç‰∏¶Âº∑Ë™øÊÑüÁü•Áõ∏ÈóúÁâπÂæµ„ÄÇÈü≥Ë®äÊ©üÂô®Â≠∏Áøí‰∏≠ÁöÑÊ®ôÊ∫ñÂÅöÊ≥ïÔºåÁî®ÊñºÊîπÂñÑÊ®°ÂûãÊî∂ÊñÇÂíåÊÄßËÉΩ„ÄÇ">
                            Mel-spectrograms converted to decibel (dB) scale using log transformation, normalizing dynamic range and emphasizing perceptually relevant features. Standard practice in audio ML for improved model convergence and performance.
                        </p>
                    </div>
                </div>
            </section>

            <section class="content-section">
                <h2 class="section-title" data-en="Model Training & Optimization" data-zh="Ê®°ÂûãË®ìÁ∑¥ËàáÂÑ™Âåñ">Model Training & Optimization</h2>
                <ul class="highlights-list">
                    <li data-en="Hardware: NVIDIA GeForce RTX 3060 Laptop GPU (6.44GB VRAM) with CUDA 11.8 acceleration" data-zh="Á°¨È´îÔºöNVIDIA GeForce RTX 3060 Á≠ÜË®òÂûãÈõªËÖ¶ GPUÔºà6.44GB VRAMÔºâÈÖçÂêà CUDA 11.8 Âä†ÈÄü">
                        <strong>Hardware:</strong> NVIDIA GeForce RTX 3060 Laptop GPU (6.44GB VRAM) with CUDA 11.8 acceleration
                    </li>
                    <li data-en="Optimizer: Adam with learning rate 0.001 and weight decay 1e-5 for L2 regularization" data-zh="ÂÑ™ÂåñÂô®ÔºöAdamÔºåÂ≠∏ÁøíÁéá 0.001ÔºåÊ¨äÈáçË°∞Ê∏õ 1e-5 Áî®Êñº L2 Ê≠£ÂâáÂåñ">
                        <strong>Optimizer:</strong> Adam with learning rate 0.001 and weight decay 1e-5 for L2 regularization
                    </li>
                    <li data-en="Scheduler: ReduceLROnPlateau (factor=0.5, patience=5) dynamically reduces learning rate when validation loss plateaus" data-zh="Ë™øÂ∫¶Âô®ÔºöReduceLROnPlateauÔºàÂõ†Â≠ê=0.5ÔºåËÄêÂøÉ=5ÔºâÂú®È©óË≠âÊêçÂ§±ÂÅúÊªØÊôÇÂãïÊÖãÈôç‰ΩéÂ≠∏ÁøíÁéá">
                        <strong>Scheduler:</strong> ReduceLROnPlateau (factor=0.5, patience=5) dynamically reduces learning rate when validation loss plateaus
                    </li>
                    <li data-en="Early Stopping: Patience of 35 epochs prevents overfitting by halting training when validation accuracy stops improving" data-zh="Êó©ÂÅúÔºö35 ÂÄã epoch ÁöÑËÄêÂøÉÈÄèÈÅéÂú®È©óË≠âÊ∫ñÁ¢∫ÁéáÂÅúÊ≠¢ÊîπÂñÑÊôÇÂÅúÊ≠¢Ë®ìÁ∑¥‰æÜÈò≤Ê≠¢ÈÅéÊì¨Âêà">
                        <strong>Early Stopping:</strong> Patience of 35 epochs prevents overfitting by halting training when validation accuracy stops improving
                    </li>
                    <li data-en="Batch Size: 32 samples per batch, optimized for GPU memory and training stability" data-zh="ÊâπÊ¨°Â§ßÂ∞èÔºöÊØèÊâπÊ¨° 32 ÂÄãÊ®£Êú¨ÔºåÈáùÂ∞ç GPU Ë®òÊÜ∂È´îÂíåË®ìÁ∑¥Á©©ÂÆöÊÄßÂÑ™Âåñ">
                        <strong>Batch Size:</strong> 32 samples per batch, optimized for GPU memory and training stability
                    </li>
                    <li data-en="Loss Function: Cross-entropy loss for multi-class classification with balanced class weights" data-zh="ÊêçÂ§±ÂáΩÊï∏ÔºöÁî®ÊñºÂ§öÈ°ûÂà•ÂàÜÈ°ûÁöÑ‰∫§ÂèâÁÜµÊêçÂ§±ÔºåÂÖ∑ÊúâÂπ≥Ë°°È°ûÂà•Ê¨äÈáç">
                        <strong>Loss Function:</strong> Cross-entropy loss for multi-class classification with balanced class weights
                    </li>
                </ul>
            </section>

            <section class="content-section">
                <h2 class="section-title" data-en="Performance Results" data-zh="ÊÄßËÉΩÁµêÊûú">Performance Results</h2>
                <div class="results-grid">
                    <div class="result-card"><div class="result-number">87.41%</div><div class="result-label" data-en="Test Accuracy" data-zh="Ê∏¨Ë©¶Ê∫ñÁ¢∫Áéá">Test Accuracy</div></div>
                    <div class="result-card"><div class="result-number">2.75M</div><div class="result-label" data-en="Parameters" data-zh="ÂèÉÊï∏">Parameters</div></div>
                    <div class="result-card"><div class="result-number">5x</div><div class="result-label" data-en="Augmentation" data-zh="Êï∏ÊìöÂ¢ûÂº∑">Augmentation</div></div>
                    <div class="result-card"><div class="result-number">+9.6%</div><div class="result-label" data-en="vs Baseline" data-zh="Ë∂ÖË∂äÂü∫Ê∫ñ">vs Baseline</div></div>
                </div>
            </section>

            <section class="content-section">
                <h2 class="section-title" data-en="Per-Class Performance Analysis" data-zh="ÂêÑÈ°ûÂà•ÊÄßËÉΩÂàÜÊûê">Per-Class Performance Analysis</h2>
                <p class="content-text" data-en="The CNN-LSTM model demonstrates strong performance across all emotion classes with macro-averaged F1-score of 0.873:" data-zh="CNN-LSTM Ê®°ÂûãÂú®ÊâÄÊúâÊÉÖÁ∑íÈ°ûÂà•‰∏≠Â±ïÁèæÂº∑ÂãÅÊÄßËÉΩÔºåÂÆèÂπ≥Âùá F1 ÂàÜÊï∏ÁÇ∫ 0.873Ôºö">
                    The CNN-LSTM model demonstrates strong performance across all emotion classes with macro-averaged F1-score of 0.873:
                </p>
                <div class="features-grid">
                    <div class="feature-card">
                        <h4 style="color: #667eea;">Emotion 1: 89.29% Recall</h4>
                        <p data-en="Strong classification with 86.21% precision and 87.72% F1-score across 28 test samples, indicating reliable detection with minimal false positives." data-zh="Âú® 28 ÂÄãÊ∏¨Ë©¶Ê®£Êú¨‰∏≠Ôºå‰ª• 86.21% Á≤æÁ¢∫ÁéáÂíå 87.72% F1 ÂàÜÊï∏ÈÄ≤Ë°åÂº∑ÂãÅÂàÜÈ°ûÔºåÈ°ØÁ§∫ÂèØÈù†ÁöÑÊ™¢Ê∏¨‰∏îË™§Âà§Ê•µÂ∞ë„ÄÇ">
                            Strong classification with 86.21% precision and 87.72% F1-score across 28 test samples, indicating reliable detection with minimal false positives.
                        </p>
                    </div>
                    <div class="feature-card">
                        <h4 style="color: #667eea;">Emotion 2: 85.19% Recall</h4>
                        <p data-en="Excellent precision (92.00%) with 88.46% F1-score on 27 samples. High precision suggests model is highly confident when predicting this class." data-zh="Âú® 27 ÂÄãÊ®£Êú¨‰∏äÂÖ∑ÊúâÂá∫Ëâ≤ÁöÑÁ≤æÁ¢∫ÁéáÔºà92.00%ÔºâÂíå 88.46% F1 ÂàÜÊï∏„ÄÇÈ´òÁ≤æÁ¢∫ÁéáË°®ÊòéÊ®°ÂûãÂú®È†êÊ∏¨Ê≠§È°ûÂà•ÊôÇÈùûÂ∏∏Êúâ‰ø°ÂøÉ„ÄÇ">
                            Excellent precision (92.00%) with 88.46% F1-score on 27 samples. High precision suggests model is highly confident when predicting this class.
                        </p>
                    </div>
                    <div class="feature-card">
                        <h4 style="color: #667eea;">Emotion 3: 74.07% Recall</h4>
                        <p data-en="Most challenging class with 95.24% precision but lower recall (74.07%). High precision indicates predictions are accurate when made, but some samples are missed." data-zh="ÊúÄÂÖ∑ÊåëÊà∞ÊÄßÁöÑÈ°ûÂà•ÔºåÁ≤æÁ¢∫ÁéáÁÇ∫ 95.24% ‰ΩÜÂè¨ÂõûÁéáËºÉ‰ΩéÔºà74.07%Ôºâ„ÄÇÈ´òÁ≤æÁ¢∫ÁéáË°®Á§∫È†êÊ∏¨Ê∫ñÁ¢∫Ôºå‰ΩÜÊúÉÈÅ∫Êºè‰∏Ä‰∫õÊ®£Êú¨„ÄÇ">
                            Most challenging class with 95.24% precision but lower recall (74.07%). High precision indicates predictions are accurate when made, but some samples are missed.
                        </p>
                    </div>
                    <div class="feature-card">
                        <h4 style="color: #667eea;">Emotion 4: 96.15% Recall</h4>
                        <p data-en="Best performing class with 83.33% precision and 89.29% F1-score. Near-perfect recall (96.15%) shows model captures almost all instances of this emotion." data-zh="Ë°®ÁèæÊúÄ‰Ω≥ÁöÑÈ°ûÂà•ÔºåÁ≤æÁ¢∫Áéá 83.33%ÔºåF1 ÂàÜÊï∏ 89.29%„ÄÇËøë‰πéÂÆåÁæéÁöÑÂè¨ÂõûÁéáÔºà96.15%ÔºâÈ°ØÁ§∫Ê®°ÂûãÊçïÊçâÂà∞Ê≠§ÊÉÖÁ∑íÁöÑÂπæ‰πéÊâÄÊúâÂØ¶‰æã„ÄÇ">
                            Best performing class with 83.33% precision and 89.29% F1-score. Near-perfect recall (96.15%) shows model captures almost all instances of this emotion.
                        </p>
                    </div>
                    <div class="feature-card">
                        <h4 style="color: #667eea;">Emotion 5: 92.59% Recall</h4>
                        <p data-en="Strong performance with 83.33% precision and 87.72% F1-score on 27 samples. Balanced precision-recall trade-off indicates robust classification." data-zh="Âú® 27 ÂÄãÊ®£Êú¨‰∏äÔºå‰ª• 83.33% Á≤æÁ¢∫ÁéáÂíå 87.72% F1 ÂàÜÊï∏Â±ïÁèæÂº∑ÂãÅÊÄßËÉΩ„ÄÇÂπ≥Ë°°ÁöÑÁ≤æÁ¢∫Áéá-Âè¨ÂõûÁéáÊ¨äË°°È°ØÁ§∫Âº∑ÂÅ•ÁöÑÂàÜÈ°û„ÄÇ">
                            Strong performance with 83.33% precision and 87.72% F1-score on 27 samples. Balanced precision-recall trade-off indicates robust classification.
                        </p>
                    </div>
                    <div class="feature-card">
                        <h4 style="color: #10b981;">Confusion Matrix Insights</h4>
                        <p data-en="Highest confusion occurs between Emotion 3 and neutral emotions, reflecting acoustic similarity. Model maintains >74% recall across all classes, demonstrating consistent performance." data-zh="Emotion 3 Âíå‰∏≠ÊÄßÊÉÖÁ∑í‰πãÈñìÁöÑÊ∑∑Ê∑ÜÊúÄÈ´òÔºåÂèçÊò†ËÅ≤Â≠∏Áõ∏‰ººÊÄß„ÄÇÊ®°ÂûãÂú®ÊâÄÊúâÈ°ûÂà•‰∏≠‰øùÊåÅ >74% Âè¨ÂõûÁéáÔºåÂ±ïÁèæ‰∏ÄËá¥ÁöÑÊÄßËÉΩ„ÄÇ">
                            Highest confusion occurs between Emotion 3 and neutral emotions, reflecting acoustic similarity. Model maintains >74% recall across all classes, demonstrating consistent performance.
                        </p>
                    </div>
                </div>
            </section>

            <section class="content-section">
                <h2 class="section-title" data-en="Baseline Comparison" data-zh="Âü∫Ê∫ñÊØîËºÉ">Baseline Comparison</h2>
                <p class="content-text" data-en="CNN-LSTM significantly outperformed traditional machine learning approaches on the same dataset:" data-zh="CNN-LSTM Âú®Âêå‰∏ÄÊï∏ÊìöÈõÜ‰∏äÈ°ØËëóÂÑ™ÊñºÂÇ≥Áµ±Ê©üÂô®Â≠∏ÁøíÊñπÊ≥ïÔºö">
                    CNN-LSTM significantly outperformed traditional machine learning approaches on the same dataset:
                </p>
                <div class="features-grid">
                    <div class="feature-card">
                        <h4 data-en="Random Forest (Optimized)" data-zh="Èö®Ê©üÊ£ÆÊûóÔºàÂÑ™ÂåñÔºâ">üå≤ Random Forest (Optimized)</h4>
                        <p data-en="150 trees, max depth 25, entropy criterion. Achieved 77.78% test accuracy using 1024 engineered features (MFCCs, pitch, energy, zero-crossing rate). GridSearchCV hyperparameter tuning improved from baseline 71.11%." data-zh="150 Ê£µÊ®πÔºåÊúÄÂ§ßÊ∑±Â∫¶ 25ÔºåÁÜµÊ®ôÊ∫ñ„ÄÇ‰ΩøÁî® 1024 ÂÄãÂ∑•Á®ãÁâπÂæµÔºàMFCCs„ÄÅÈü≥È´ò„ÄÅËÉΩÈáè„ÄÅÈÅéÈõ∂ÁéáÔºâÈÅîÂà∞ 77.78% Ê∏¨Ë©¶Ê∫ñÁ¢∫Áéá„ÄÇGridSearchCV Ë∂ÖÂèÉÊï∏Ë™øÂÑ™ÂæûÂü∫Ê∫ñ 71.11% ÊîπÂñÑ„ÄÇ">
                            150 trees, max depth 25, entropy criterion. Achieved 77.78% test accuracy using 1024 engineered features (MFCCs, pitch, energy, zero-crossing rate). GridSearchCV hyperparameter tuning improved from baseline 71.11%.
                        </p>
                    </div>
                    <div class="feature-card">
                        <h4 data-en="Gradient Boosting" data-zh="Ê¢ØÂ∫¶ÊèêÂçá">üìà Gradient Boosting</h4>
                        <p data-en="Alternative tree-based ensemble achieving 70.56% accuracy. Despite iterative boosting approach, unable to match Random Forest performance, suggesting feature representation limitations." data-zh="Êõø‰ª£ÁöÑÂü∫ÊñºÊ®πÁöÑÈõÜÊàêÈÅîÂà∞ 70.56% Ê∫ñÁ¢∫Áéá„ÄÇÂÑòÁÆ°Êé°Áî®Ëø≠‰ª£ÊèêÂçáÊñπÊ≥ïÔºåÁÑ°Ê≥ïÂåπÈÖçÈö®Ê©üÊ£ÆÊûóÊÄßËÉΩÔºåÈ°ØÁ§∫ÁâπÂæµË°®Á§∫ÁöÑÈôêÂà∂„ÄÇ">
                            Alternative tree-based ensemble achieving 70.56% accuracy. Despite iterative boosting approach, unable to match Random Forest performance, suggesting feature representation limitations.
                        </p>
                    </div>
                    <div class="feature-card">
                        <h4 data-en="Support Vector Machine" data-zh="ÊîØÊè¥ÂêëÈáèÊ©ü">üéØ Support Vector Machine</h4>
                        <p data-en="RBF kernel SVM reached 62.22% accuracy. Lower performance indicates traditional kernel methods struggle with high-dimensional audio features compared to ensemble and deep learning approaches." data-zh="RBF Ê†∏ÂøÉ SVM ÈÅîÂà∞ 62.22% Ê∫ñÁ¢∫Áéá„ÄÇËºÉ‰ΩéÁöÑÊÄßËÉΩË°®ÊòéÂÇ≥Áµ±Ê†∏ÂøÉÊñπÊ≥ïÂú®È´òÁ∂≠Èü≥Ë®äÁâπÂæµ‰∏äË°®Áèæ‰∏çÂ¶ÇÈõÜÊàêÂíåÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ï„ÄÇ">
                            RBF kernel SVM reached 62.22% accuracy. Lower performance indicates traditional kernel methods struggle with high-dimensional audio features compared to ensemble and deep learning approaches.
                        </p>
                    </div>
                    <div class="feature-card">
                        <h4 style="color: #10b981;" data-en="CNN-LSTM Advantage" data-zh="CNN-LSTM ÂÑ™Âã¢">‚ú® CNN-LSTM Advantage</h4>
                        <p data-en="9.6% improvement over best baseline (Random Forest 77.78% ‚Üí CNN-LSTM 87.41%). Deep learning captures complex spectrogram patterns and temporal dependencies that handcrafted features miss, justifying increased model complexity." data-zh="ÊØîÊúÄ‰Ω≥Âü∫Ê∫ñÊèêÂçá 9.6%ÔºàÈö®Ê©üÊ£ÆÊûó 77.78% ‚Üí CNN-LSTM 87.41%Ôºâ„ÄÇÊ∑±Â∫¶Â≠∏ÁøíÊçïÊçâÊâãÂ∑•ÁâπÂæµÈÅ∫ÊºèÁöÑË§áÈõúÈ†ªË≠úÂúñÊ®°ÂºèÂíåÊôÇÂ∫è‰æùË≥¥ÔºåË≠âÊòéÂ¢ûÂä†Ê®°ÂûãË§áÈõúÂ∫¶ÁöÑÂêàÁêÜÊÄß„ÄÇ">
                            9.6% improvement over best baseline (Random Forest 77.78% ‚Üí CNN-LSTM 87.41%). Deep learning captures complex spectrogram patterns and temporal dependencies that handcrafted features miss, justifying increased model complexity.
                        </p>
                    </div>
                </div>
            </section>

            <section class="content-section">
                <h2 class="section-title" data-en="Technical Challenges & Solutions" data-zh="ÊäÄË°ìÊåëÊà∞ËàáËß£Ê±∫ÊñπÊ°à">Technical Challenges & Solutions</h2>
                <div class="features-grid">
                    <div class="feature-card">
                        <h4 style="color: #667eea;">‚ö° Challenge: Overfitting Risk</h4>
                        <p style="margin-bottom: 0.5rem;" data-en="2.75M parameters with only 630 training samples created severe overfitting risk, potentially leading to >95% train accuracy but poor generalization." data-zh="275 Ëê¨ÂèÉÊï∏ÂÉÖÊúâ 630 ÂÄãË®ìÁ∑¥Ê®£Êú¨ÈÄ†ÊàêÂö¥ÈáçÈÅéÊì¨ÂêàÈ¢®Èö™ÔºåÂèØËÉΩÂ∞éËá¥ >95% Ë®ìÁ∑¥Ê∫ñÁ¢∫Áéá‰ΩÜÊ≥õÂåñËÉΩÂäõÂ∑Æ„ÄÇ">
                            2.75M parameters with only 630 training samples created severe overfitting risk, potentially leading to >95% train accuracy but poor generalization.
                        </p>
                        <p style="color: #10b981; font-weight: 600;">Solution:</p>
                        <p data-en="Implemented multi-layer regularization: 5x data augmentation (effective 3,150 samples/epoch), 30% dropout after each major component, weight decay (1e-5), and early stopping (patience=35). Achieved ~99% train with 87.41% test accuracy." data-zh="ÂØ¶‰ΩúÂ§öÂ±§Ê≠£ÂâáÂåñÔºö5 ÂÄçÊï∏ÊìöÂ¢ûÂº∑ÔºàÊúâÊïàÊØè epoch 3,150 ÂÄãÊ®£Êú¨Ôºâ„ÄÅÊØèÂÄã‰∏ªË¶ÅÁµÑ‰ª∂Âæå 30% dropout„ÄÅÊ¨äÈáçË°∞Ê∏õÔºà1e-5ÔºâÂíåÊó©ÂÅúÔºàËÄêÂøÉ=35Ôºâ„ÄÇÈÅîÂà∞Á¥Ñ 99% Ë®ìÁ∑¥Ê∫ñÁ¢∫ÁéáÂíå 87.41% Ê∏¨Ë©¶Ê∫ñÁ¢∫Áéá„ÄÇ">
                            Implemented multi-layer regularization: 5x data augmentation (effective 3,150 samples/epoch), 30% dropout after each major component, weight decay (1e-5), and early stopping (patience=35). Achieved ~99% train with 87.41% test accuracy.
                        </p>
                    </div>
                    <div class="feature-card">
                        <h4 style="color: #667eea;">‚ö° Challenge: GPU Memory Constraints</h4>
                        <p style="margin-bottom: 0.5rem;" data-en="6.44GB VRAM limited batch size with 5x augmentation (effective 160 samples per batch) and 2.75M parameter model requiring careful memory management." data-zh="6.44GB VRAM Âú® 5 ÂÄçÂ¢ûÂº∑ÔºàÊØèÊâπÊ¨°ÊúâÊïà 160 ÂÄãÊ®£Êú¨ÔºâÂíå 275 Ëê¨ÂèÉÊï∏Ê®°Âûã‰∏ãÈôêÂà∂ÊâπÊ¨°Â§ßÂ∞èÔºåÈúÄË¶Å‰ªîÁ¥∞ÁöÑË®òÊÜ∂È´îÁÆ°ÁêÜ„ÄÇ">
                            6.44GB VRAM limited batch size with 5x augmentation (effective 160 samples per batch) and 2.75M parameter model requiring careful memory management.
                        </p>
                        <p style="color: #10b981; font-weight: 600;">Solution:</p>
                        <p data-en="Optimized batch size to 32 (160 augmented), used mixed-precision training implicitly through PyTorch optimizations, and applied gradient accumulation when needed. Maintained training stability without OOM errors." data-zh="ÂÑ™ÂåñÊâπÊ¨°Â§ßÂ∞èËá≥ 32Ôºà160 ÂÄãÂ¢ûÂº∑ÔºâÔºåÈÄèÈÅé PyTorch ÂÑ™ÂåñÈö±Âºè‰ΩøÁî®Ê∑∑ÂêàÁ≤æÂ∫¶Ë®ìÁ∑¥Ôºå‰∏¶Âú®ÈúÄË¶ÅÊôÇÊáâÁî®Ê¢ØÂ∫¶Á¥ØÁ©ç„ÄÇÁ∂≠ÊåÅË®ìÁ∑¥Á©©ÂÆöÊÄßËÄåÁÑ°Ë®òÊÜ∂È´î‰∏çË∂≥ÈåØË™§„ÄÇ">
                            Optimized batch size to 32 (160 augmented), used mixed-precision training implicitly through PyTorch optimizations, and applied gradient accumulation when needed. Maintained training stability without OOM errors.
                        </p>
                    </div>
                    <div class="feature-card">
                        <h4 style="color: #667eea;">‚ö° Challenge: Class Imbalance Sensitivity</h4>
                        <p style="margin-bottom: 0.5rem;" data-en="While dataset has balanced distribution (20% each), augmentation and sampling could introduce training bias toward certain emotion classes during mini-batch training." data-zh="ÈõñÁÑ∂Êï∏ÊìöÈõÜÂÖ∑ÊúâÂπ≥Ë°°ÂàÜÂ∏ÉÔºàÂêÑ 20%ÔºâÔºå‰ΩÜÂ¢ûÂº∑ÂíåÊé°Ê®£ÂèØËÉΩÂú®Â∞èÊâπÊ¨°Ë®ìÁ∑¥ÊúüÈñìÂ∞éËá¥Â∞çÊüê‰∫õÊÉÖÁ∑íÈ°ûÂà•ÁöÑË®ìÁ∑¥ÂÅèÂ∑Æ„ÄÇ">
                            While dataset has balanced distribution (20% each), augmentation and sampling could introduce training bias toward certain emotion classes during mini-batch training.
                        </p>
                        <p style="color: #10b981; font-weight: 600;">Solution:</p>
                        <p data-en="Used stratified train/val/test split ensuring proportional class representation across all splits. Applied uniform augmentation to all classes maintaining balance. Result: all classes achieve >74% recall with minimal variance." data-zh="‰ΩøÁî®ÂàÜÂ±§Ë®ìÁ∑¥/È©óË≠â/Ê∏¨Ë©¶ÂàÜÂâ≤Á¢∫‰øùÊâÄÊúâÂàÜÂâ≤‰∏≠ÁöÑÊØî‰æãÈ°ûÂà•Ë°®Á§∫„ÄÇÂ∞çÊâÄÊúâÈ°ûÂà•ÊáâÁî®Áµ±‰∏ÄÂ¢ûÂº∑Á∂≠ÊåÅÂπ≥Ë°°„ÄÇÁµêÊûúÔºöÊâÄÊúâÈ°ûÂà•ÈÅîÂà∞ >74% Âè¨ÂõûÁéáÔºåËÆäÁï∞ÊúÄÂ∞è„ÄÇ">
                            Used stratified train/val/test split ensuring proportional class representation across all splits. Applied uniform augmentation to all classes maintaining balance. Result: all classes achieve >74% recall with minimal variance.
                        </p>
                    </div>
                    <div class="feature-card">
                        <h4 style="color: #667eea;">‚ö° Challenge: Feature Representation</h4>
                        <p style="margin-bottom: 0.5rem;" data-en="Choosing between handcrafted features (MFCCs, pitch, energy) versus learned representations from raw spectrograms, balancing domain knowledge with model capacity." data-zh="Âú®ÊâãÂ∑•ÁâπÂæµÔºàMFCCs„ÄÅÈü≥È´ò„ÄÅËÉΩÈáèÔºâËàáÂæûÂéüÂßãÈ†ªË≠úÂúñÂ≠∏ÁøíÁöÑË°®Á§∫‰πãÈñìÈÅ∏ÊìáÔºåÂπ≥Ë°°È†òÂüüÁü•Ë≠òËàáÊ®°ÂûãËÉΩÂäõ„ÄÇ">
                            Choosing between handcrafted features (MFCCs, pitch, energy) versus learned representations from raw spectrograms, balancing domain knowledge with model capacity.
                        </p>
                        <p style="color: #10b981; font-weight: 600;">Solution:</p>
                        <p data-en="Selected mel-spectrograms as input, allowing CNN layers to learn optimal feature hierarchies automatically. Mel-scale provides perceptually-motivated frequency representation while CNN discovers emotion-relevant patterns, outperforming 1024 handcrafted features by 9.6%." data-zh="ÈÅ∏Êìá mel-spectrograms ‰ΩúÁÇ∫Ëº∏ÂÖ•ÔºåÂÖÅË®± CNN Â±§Ëá™ÂãïÂ≠∏ÁøíÊúÄ‰Ω≥ÁâπÂæµÂ±§Ê¨°„ÄÇMel Â∞∫Â∫¶Êèê‰æõÊÑüÁü•Â∞éÂêëÁöÑÈ†ªÁéáË°®Á§∫ÔºåËÄå CNN ÁôºÁèæÊÉÖÁ∑íÁõ∏ÈóúÊ®°ÂºèÔºåË∂ÖË∂ä 1024 ÂÄãÊâãÂ∑•ÁâπÂæµ 9.6%„ÄÇ">
                            Selected mel-spectrograms as input, allowing CNN layers to learn optimal feature hierarchies automatically. Mel-scale provides perceptually-motivated frequency representation while CNN discovers emotion-relevant patterns, outperforming 1024 handcrafted features by 9.6%.
                        </p>
                    </div>
                </div>
            </section>

            <section class="content-section">
                <h2 class="section-title" data-en="Applications & Impact" data-zh="ÊáâÁî®ËàáÂΩ±Èüø">Applications & Impact</h2>
                <p class="content-text" data-en="Speech emotion recognition enables various real-world applications in human-computer interaction and assistive technologies:" data-zh="Ë™ûÈü≥ÊÉÖÁ∑íË≠òÂà•Âú®‰∫∫Ê©ü‰∫íÂãïÂíåËºîÂä©ÊäÄË°ì‰∏≠ÂØ¶ÁèæÂêÑÁ®ÆÂØ¶ÈöõÊáâÁî®Ôºö">
                    Speech emotion recognition enables various real-world applications in human-computer interaction and assistive technologies:
                </p>
                <div class="features-grid">
                    <div class="feature-card">
                        <h4 data-en="Mental Health Monitoring" data-zh="ÂøÉÁêÜÂÅ•Â∫∑Áõ£Ê∏¨">üß† Mental Health Monitoring</h4>
                        <p data-en="Real-time emotion detection in therapeutic sessions can help clinicians identify emotional states, track patient progress, and detect early warning signs of mental health crises through voice analysis." data-zh="Ê≤ªÁôÇÊúÉË©±‰∏≠ÁöÑÂç≥ÊôÇÊÉÖÁ∑íÊ™¢Ê∏¨ÂèØ‰ª•Âπ´Âä©Ëá®Â∫äÈÜ´ÁîüË≠òÂà•ÊÉÖÁ∑íÁãÄÊÖã„ÄÅËøΩËπ§ÊÇ£ËÄÖÈÄ≤Â±ïÔºå‰∏¶ÈÄèÈÅéË™ûÈü≥ÂàÜÊûêÊ™¢Ê∏¨ÂøÉÁêÜÂÅ•Â∫∑Âç±Ê©üÁöÑÊó©ÊúüÈ†êË≠¶‰ø°Ëôü„ÄÇ">
                            Real-time emotion detection in therapeutic sessions can help clinicians identify emotional states, track patient progress, and detect early warning signs of mental health crises through voice analysis.
                        </p>
                    </div>
                    <div class="feature-card">
                        <h4 data-en="Call Center Analytics" data-zh="ÂÆ¢Êúç‰∏≠ÂøÉÂàÜÊûê">üìû Call Center Analytics</h4>
                        <p data-en="Automated sentiment analysis of customer service calls enables quality monitoring, agent performance evaluation, and early escalation of frustrated customers to supervisors, improving customer satisfaction." data-zh="ÂÆ¢Êà∂ÊúçÂãôÈÄöË©±ÁöÑËá™ÂãïÊÉÖÊÑüÂàÜÊûêÂØ¶ÁèæÂìÅË≥™Áõ£Êéß„ÄÅÂÆ¢Êúç‰∫∫Âì°Á∏æÊïàË©ï‰º∞Ôºå‰ª•ÂèäÂ∞áÂèóÊå´ÁöÑÂÆ¢Êà∂Êó©ÊúüÂçáÁ¥öÁµ¶‰∏ªÁÆ°ÔºåÊèêÈ´òÂÆ¢Êà∂ÊªøÊÑèÂ∫¶„ÄÇ">
                            Automated sentiment analysis of customer service calls enables quality monitoring, agent performance evaluation, and early escalation of frustrated customers to supervisors, improving customer satisfaction.
                        </p>
                    </div>
                    <div class="feature-card">
                        <h4 data-en="Accessibility Tools" data-zh="ÁÑ°ÈöúÁ§ôÂ∑•ÂÖ∑">‚ôø Accessibility Tools</h4>
                        <p data-en="Assists individuals with autism spectrum disorder or social communication difficulties by providing real-time emotional cues from speech, enhancing their ability to navigate social interactions." data-zh="ÈÄèÈÅéÊèê‰æõË™ûÈü≥ÁöÑÂç≥ÊôÇÊÉÖÁ∑íÊèêÁ§∫ÔºåÂπ´Âä©Ëá™ÈñâÁóáË≠úÁ≥ªÈöúÁ§ôÊàñÁ§æ‰∫§Ê∫ùÈÄöÂõ∞Èõ£ÁöÑÂÄã‰∫∫ÔºåÂ¢ûÂº∑‰ªñÂÄëÊáâÂ∞çÁ§æ‰∫§‰∫íÂãïÁöÑËÉΩÂäõ„ÄÇ">
                            Assists individuals with autism spectrum disorder or social communication difficulties by providing real-time emotional cues from speech, enhancing their ability to navigate social interactions.
                        </p>
                    </div>
                    <div class="feature-card">
                        <h4 data-en="Human-Robot Interaction" data-zh="‰∫∫Ê©ü‰∫íÂãï">ü§ñ Human-Robot Interaction</h4>
                        <p data-en="Empowers robots and virtual assistants to recognize user emotions and adapt responses accordingly, creating more natural and empathetic interactions in smart home devices, educational robots, and companion systems." data-zh="Ë≥¶‰∫àÊ©üÂô®‰∫∫ÂíåËôõÊì¨Âä©ÁêÜË≠òÂà•‰ΩøÁî®ËÄÖÊÉÖÁ∑í‰∏¶Áõ∏ÊáâË™øÊï¥ÂõûÊáâÁöÑËÉΩÂäõÔºåÂú®Êô∫ÊÖßÂÆ∂Â±ÖË®≠ÂÇô„ÄÅÊïôËÇ≤Ê©üÂô®‰∫∫ÂíåÈô™‰º¥Á≥ªÁµ±‰∏≠ÂâµÈÄ†Êõ¥Ëá™ÁÑ∂ÂíåÊúâÂêåÁêÜÂøÉÁöÑ‰∫íÂãï„ÄÇ">
                            Empowers robots and virtual assistants to recognize user emotions and adapt responses accordingly, creating more natural and empathetic interactions in smart home devices, educational robots, and companion systems.
                        </p>
                    </div>
                </div>
            </section>

            <section class="content-section">
                <h2 class="section-title" data-en="Technology Stack" data-zh="ÊäÄË°ìÊ£ß">Technology Stack</h2>
                <div class="tech-stack">
                    <div class="tech-item">
                        <i class="fab fa-python"></i>
                        <span>Python</span>
                    </div>
                    <div class="tech-item">
                        <i class="fas fa-fire"></i>
                        <span>PyTorch</span>
                    </div>
                    <div class="tech-item">
                        <i class="fas fa-music"></i>
                        <span>Librosa</span>
                    </div>
                    <div class="tech-item">
                        <i class="fas fa-brain"></i>
                        <span>CNN-LSTM</span>
                    </div>
                    <div class="tech-item">
                        <i class="fas fa-chart-line"></i>
                        <span>Scikit-learn</span>
                    </div>
                    <div class="tech-item">
                        <i class="fas fa-cube"></i>
                        <span>NumPy</span>
                    </div>
                </div>
            </section>
        </div>
    </div>

    <section class="links-section">
        <div class="container">
            <h2 class="section-title" data-en="Project Links" data-zh="Â∞àÊ°àÈÄ£Áµê">Project Links</h2>
            <div class="project-links">
                <a href="https://github.com/ckcinnabar/project-team-formation-intelligent-artifacts" class="btn btn-primary" target="_blank" rel="noopener noreferrer">
                    <i class="fab fa-github"></i>
                    <span data-en="View on GitHub" data-zh="Âú® GitHub ‰∏äÊü•Áúã">View on GitHub</span>
                </a>
                <a href="SpeechEmotion/poster.pdf" class="btn btn-secondary" target="_blank">
                    <i class="fas fa-file-pdf"></i>
                    <span data-en="Research Poster" data-zh="Á†îÁ©∂Êµ∑Â†±">Research Poster</span>
                </a>
            </div>
        </div>
    </section>

    <footer class="footer">
        <div class="container"><p>&copy; 2025 Kuan-Chen Chen. <span data-en="All rights reserved." data-zh="ÁâàÊ¨äÊâÄÊúâ„ÄÇ">All rights reserved.</span></p></div>
    </footer>
    <script src="../script.js"></script>
</body>
</html>
